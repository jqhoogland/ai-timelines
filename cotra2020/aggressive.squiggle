// Based on Ajeya Cotra's 2020 "When compute required to train a transformative model may be attainable".
// - https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines
// 
// Adapted by: Jesse Hoogland

// Best guess 
// - https://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid=505210495

// Model parameters
compute$HalvingTime = 1.5
maxFLOPPer$ = 1e26
growthRate = .05
spendDoublingTime = 1
maxFracGWP = .02

// Fixed
startYear = 2025
endYear = 2100

flopPer$(y) = {
    currFLOPPer$ = 4e17
    t = y - startYear
    currFLOPPer$ * (exp((log(2) / compute$HalvingTime) * t)) / (1 + currFLOPPer$ / maxFLOPPer$ * exp(log(2)/compute$HalvingTime * t))  
}

frontierCountryGDP(y) = {
    // Our reference measurement comes from 2020, not 2025.
    t = y - 2020
    frontierCountryGDP2020 = 2.13e13
    frontierCountryGDP2020 * (1 + growthRate)^(t)
}

spend(y) = {
    currSpend = 1e9 // in 2020 USD  1e8 to 1e10
    currFracGWP = currSpend / frontierCountryGDP(startYear)

    t = y - startYear
    currSpend * (exp(log(2)/ spendDoublingTime * t)) / (1 + currFracGWP / maxFracGWP * exp(log(2) / spendDoublingTime * t))
}

flop(y) = {
    flopPer$(y) * spend(y)
}

log10Flop(y) = log10(flop(y))

// NOTE: All of the following are very coarse-grained approximations of Cotra's original estimates:
// https://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid=0
// See also ./compute-levels.squiggle (when squiggle introduces imports, we'll use those models here)

// Evolution anchor
//  "From earliest animals with neurons to modern humans"
//  ~1e41 FLOP
evolutionAnchorPDF = 34 to 47
evolutionAnchor(y) = cdf(evolutionAnchorPDF, log10Flop(y))

// Lifetime anchor 
//  "By a child's brain over the course of growing to be an adult"
//  ~1e24 FLOP
lifetimeAnchorPDF = 25 to 35 // 90% confidence
lifetimeAnchor(y) = cdf(lifetimeAnchorPDF, log10Flop(y))

// Neural network anchor
// "have about as many parameters as we would expect if we simply scaled up 
// the architectures of the largest current neural networks to run on 
// [as many] FLOP / subj sec. [as the human brain (~1e15 FLOP/s)].
nnShortAnchorPDF = 26 to 41
nnMediumAnchorPDF = 28 to 44
nnLongAnchorPDF = 30 to 46

nnShortAnchor(y) = cdf(nnShortAnchorPDF, log10Flop(y))
nnMediumAnchor(y) = cdf(nnMediumAnchorPDF, log10Flop(y))
nnLongAnchor(y) = cdf(nnLongAnchorPDF, log10Flop(y))

// Genome Anchor 
//  "have about as many parameters as there are bytes in the human genome (~7.5e8 bytes)"
genomeAnchorPDF = 28 to 40
genomeAnchor(y) = cdf(genomeAnchorPDF, log10Flop(y))

// Combined model
mixedPDF = mixture(
    lifetimeAnchorPDF,
    nnShortAnchorPDF,
    genomeAnchorPDF,
    nnMediumAnchorPDF,
    nnLongAnchorPDF,
    evolutionAnchorPDF,
    [
        .1,
        .4,
        .2,
        .1,
        .1,
        .05
    ]    
)

mixed(y) = cdf(mixedPDF, log10Flop(y))